{"0": {
    "doc": "Association Rule Mining <b>(ARM)</b>",
    "title": "Association Rule Mining (ARM)",
    "content": ". | Overview | Data | Code | Results | Conclusion | . ",
    "url": "/docs/CSCI%205622%20-%20ML/models/arm/#association-rule-mining-arm",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/arm/#association-rule-mining-arm"
  },"1": {
    "doc": "Association Rule Mining <b>(ARM)</b>",
    "title": "Overview",
    "content": "Association Rule Mining (ARM) is a method used to discover interesting relations between variables in large transaction type databases. It involves identifying patterns and associations between items in a dataset, which can be used to make predictions and/or recommendations. The most common application of ARM is in market basket analysis, where the goal is to identify items that are frequently purchased together. This information can be used to optimize product placement, marketing strategies, and inventory management. Some key concepts in ARM include: . | Support: The proportion of transactions that contain a particular itemset. It is calculated as the number of transactions containing the itemset divided by the total number of transactions. | Confidence: The likelihood that an item B is purchased given that item A is purchased. It is calculated as the number of transactions containing both items A and B divided by the number of transactions containing item A. | Lift: The ratio of the observed support to the expected support if the two items were independent. It is calculated as the confidence of the rule divided by the support of item B. | . The network graph above shows associations between selected items. Larger circles imply higher support, while red circles imply higher lift . The Apriori algorithm is a popular algorithm used for ARM. It is based on the principle that if an itemset is frequent, then all of its subsets must also be frequent. The algorithm works by iteratively generating candidate itemsets and pruning those that do not meet the minimum support threshold. The process continues until no more frequent itemsets can be found. Once frequent item sets are identified, the algorithm generates association rules by considering all possible subsets of these item sets. Finally, by analyzing these rules, you can identify strong associations between items and make informed decisions based on these patterns. FlowChart for Typical Apriori Algorithm . References: [1]: https://www.youtube.com/watch?v=WGlMlS_Yydk&amp;t [2]: https://towardsdatascience.com/data-mining-market-basket-analysis-with-apriori-algorithm-970ff256a92c [3]: https://www.datacamp.com/tutorial/association-rule-mining-python [4]: https://gatesboltonanalytics.com/?page_id=266 [5]: https://www.researchgate.net/publication/265051526_Efficient_Ordering_Policy_for_Imperfect_Quality_Items_Using_Association_Rule_Mining . ",
    "url": "/docs/CSCI%205622%20-%20ML/models/arm/#overview",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/arm/#overview"
  },"2": {
    "doc": "Association Rule Mining <b>(ARM)</b>",
    "title": "Data",
    "content": "The data is not scaled here. So the dataset looks like: . Material Lay-up Vf, % Resin Name Resin 0 Deg fabric 90 deg fabric Cure / Post Cure Process Coupon Max. Stress, MPa Min. Stress, MPa R-value Freq., Hz E, GPa Max. % Strain Min. % Strain Cycles Resin Type 7 LCCF-T20 [0]3 50.000 EP1 Hexion MGS RIMR135/RIMH1366 Taekwang T20C NaN 24h/20C, 12h/ 70C Infusion T20-400 943.000 NaN static 0.025 126.000 0.720 NaN 1.000 Epoxy 8 LCCF-T20 [0]3 49.000 EP1 Hexion MGS RIMR135/RIMH1366 Taekwang T20C NaN 24h/20C, 12h/ 70C Infusion T20-401 910.000 NaN static 0.025 125.000 0.710 NaN 1.000 Epoxy 9 LCCF-T20 [0]3 51.000 EP1 Hexion MGS RIMR135/RIMH1366 Taekwang T20C NaN 24h/20C, 12h/ 70C Infusion T20-402 966.000 NaN static 0.025 128.000 0.730 NaN 1.000 Epoxy 10 LCCF-T20 [0]3 52.000 EP1 Hexion MGS RIMR135/RIMH1366 Taekwang T20C NaN 24h/20C, 12h/ 70C Infusion T20-403 967.000 NaN static 0.025 131.000 0.710 NaN 1.000 Epoxy 11 LCCF-T20 [0]3 50.000 EP1 Hexion MGS RIMR135/RIMH1366 Taekwang T20C NaN 24h/20C, 12h/ 70C Infusion T20-404 1066.000 NaN static 0.025 125.000 0.810 NaN 1.000 Epoxy 12 LCCF-T20 [0]3 51.000 EP1 Hexion MGS RIMR135/RIMH1366 Taekwang T20C NaN 24h/20C, 12h/ 70C Infusion T20-405 972.000 NaN static 0.025 130.000 0.690 NaN 1.000 Epoxy 13 LCCF-T20 [0]3 52.000 EP1 Hexion MGS RIMR135/RIMH1366 Taekwang T20C NaN 24h/20C, 12h/ 70C Infusion T20-406 1067.000 NaN static 0.025 131.000 0.780 NaN 1.000 Epoxy 14 LCCF-T20 [0]3 51.000 EP1 Hexion MGS RIMR135/RIMH1366 Taekwang T20C NaN 24h/20C, 12h/ 70C Infusion T20-407 939.000 NaN static 0.025 134.000 0.710 NaN 1.000 Epoxy 15 LCCF-T20 [0]3 49.000 EP1 Hexion MGS RIMR135/RIMH1366 Taekwang T20C NaN 24h/20C, 12h/ 70C Infusion T20-408 926.000 NaN static 0.025 124.000 0.720 NaN 1.000 Epoxy 16 LCCF-T20 [0]3 47.000 EP1 Hexion MGS RIMR135/RIMH1366 Taekwang T20C NaN 24h/20C, 12h/ 70C Infusion T20-409 1035.000 NaN static 0.025 119.000 0.830 NaN 1.000 Epoxy . The data is now divided into two sets: categorical and numerical. The categorical data is as follows: . categorical_cols are: ['Material', 'Lay-up', 'Resin Name', '0 Deg fabric', '90 deg fabric', 'Cure / Post Cure', 'Process', 'Coupon', 'Resin Type'] quantitative_cols are: ['Vf, %', 'Max. Stress, MPa', 'Min. Stress, MPa', 'E, GPa', 'Max. % Strain', 'Min. % Strain',] . Some columns like R-value, Cycles, etc. have been dropped as they provide repetitive information for this use case. The categorical data is now encoded using the get_dummies from pandas library. Vf, % Max. Stress, MPa Min. Stress, MPa E, GPa Max. % Strain Min. % Strain Material_LCCF-K20 Material_LCCF-T20 Material_UNI-AGY S1HM-EP1 Material_UNI-ELM1810-UP3 Material_UNI-ELM3610-UP3 Material_UNI-ELT5500-EP1 Material_UNI-ELT5500-EP3 Material_UNI-ELT5500-UP3 Material_UNI-ELT5500-UP5 Material_UNI-ELT5500-VE4 Material_UNI-Neptco Rodpack Material_UNI-OCV1000-EP1 Material_UNI-OCV1200-EP1 Material_UNI-OCV1322-EP1 Material_UNI-PPG1200-EP1 Material_UNI-PPG1200-EP5 Material_UNI-PPG1200-PU Material_UNI-PPG1200-UP5 Material_UNI-PPG1200-VE2 Material_UNI-PPG1200-VE4 Material_UNI-PPG1250-EP1 Material_UNI-PPG1250-EP10 Material_UNI-PPG1250-EP5 Material_UNI-PPG1250-EP9 Material_UNI-PPGHYBON2400-EP5 Material_UNI-PPGHYBON2400-UP5 Material_UNI-PPGHYBON2400-VE4 Material_UNI-PPGHYBON2400-VE5 Material_UNI-PPGHYBON2400-VE6 Material_UNI-PPGHYBON4400-VE4 Material_Zoltek PX35 Lay-up_[0]2 Lay-up_[0]20 Lay-up_[0]3 Lay-up_[0]4 Lay-up_[0]5 Lay-up_[0]6 Lay-up_[90] Lay-up_[90]2 Lay-up_[90]3 Lay-up_[90]4 Lay-up_[90]5 Resin Name_EP-10 Resin Name_EP-9 Resin Name_EP1 Resin Name_EP3 Resin Name_EP5 Resin Name_Epoxy Resin Name_PU Resin Name_UP3 Resin Name_UP5 Resin Name_VE Resin Name_VE4 Resin Name_VE5 Resin Name_VE6 0 Deg fabric_AGY 1250 gsm/2400 tex S-1 HM roving (E90GPa) 0 Deg fabric_Aligned Strand, PPG Hybon 2026 0 Deg fabric_Aligned strands 0 Deg fabric_Kaltex K20-HTU 0 Deg fabric_OCV Advantex 1000UD (1000 gsm) 0 Deg fabric_OCV Advantex 1200UD (1200 gsm) 0 Deg fabric_OCV Advantex 1322UD (1322 gsm) 0 Deg fabric_PPG-Devold L1200/G50-E07 0 Deg fabric_PPG-Devold L1200/G50-E07/2026 0 Deg fabric_SparPreg UC600, 600 gsm 0 Deg fabric_Taekwang T20C 0 Deg fabric_Vectorply E-LM-1810 0 Deg fabric_Vectorply E-LM-3610 0 Deg fabric_Vectorply E-LT-5500 0 Deg fabric_Vectorply ELT-5500 0 Deg fabric_Zoltek PX3505015T-13 90 deg fabric_8 90 deg fabric_100 Cure / Post Cure_24h/20C, 12h/80 C Cure / Post Cure_24h/20C, 3h/ 65 C \\ 7 50.000 943.000 NaN 126.000 0.720 NaN False True False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False False True False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False 8 49.000 910.000 NaN 125.000 0.710 NaN False True False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False False True False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False 9 51.000 966.000 NaN 128.000 0.730 NaN False True False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False False True False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False 10 52.000 967.000 NaN 131.000 0.710 NaN False True False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False False True False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False 11 50.000 1066.000 NaN 125.000 0.810 NaN False True False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False False True False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False 12 51.000 972.000 NaN 130.000 0.690 NaN False True False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False False True False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False 13 52.000 1067.000 NaN 131.000 0.780 . The numerical data is binned into 3 categories using the pd.cut function from the pandas library. The categories are LOW, MED, and HIGH. Finally, the two datasets are concatenated to form the final, completely encoded dataset. A sample of the final dataset is as follows: . Vf, % Max. Stress, MPa Min. Stress, MPa E, GPa Max. % Strain Min. % Strain Material_LCCF-K20 Material_LCCF-T20 Material_UNI-AGY S1HM-EP1 Material_UNI-ELM1810-UP3 Material_UNI-ELM3610-UP3 Material_UNI-ELT5500-EP1 Material_UNI-ELT5500-EP3 Material_UNI-ELT5500-UP3 Material_UNI-ELT5500-UP5 Material_UNI-ELT5500-VE4 Material_UNI-Neptco Rodpack Material_UNI-OCV1000-EP1 Material_UNI-OCV1200-EP1 Material_UNI-OCV1322-EP1 Material_UNI-PPG1200-EP1 Material_UNI-PPG1200-EP5 Material_UNI-PPG1200-PU Material_UNI-PPG1200-UP5 Material_UNI-PPG1200-VE2 Material_UNI-PPG1200-VE4 Material_UNI-PPG1250-EP1 Material_UNI-PPG1250-EP10 Material_UNI-PPG1250-EP5 Material_UNI-PPG1250-EP9 Material_UNI-PPGHYBON2400-EP5 Material_UNI-PPGHYBON2400-UP5 Material_UNI-PPGHYBON2400-VE4 Material_UNI-PPGHYBON2400-VE5 Material_UNI-PPGHYBON2400-VE6 Material_UNI-PPGHYBON4400-VE4 Material_Zoltek PX35 Lay-up_[0]2 Lay-up_[0]20 Lay-up_[0]3 Lay-up_[0]4 Lay-up_[0]5 Lay-up_[0]6 Lay-up_[90] Lay-up_[90]2 Lay-up_[90]3 Lay-up_[90]4 Lay-up_[90]5 Resin Name_EP-10 Resin Name_EP-9 Resin Name_EP1 Resin Name_EP3 Resin Name_EP5 Resin Name_Epoxy Resin Name_PU Resin Name_UP3 Resin Name_UP5 Resin Name_VE Resin Name_VE4 Resin Name_VE5 Resin Name_VE6 0 Deg fabric_AGY 1250 gsm/2400 tex S-1 HM roving (E90GPa) 0 Deg fabric_Aligned Strand, PPG Hybon 2026 0 Deg fabric_Aligned strands 0 Deg fabric_Kaltex K20-HTU 0 Deg fabric_OCV Advantex 1000UD (1000 gsm) 0 Deg fabric_OCV Advantex 1200UD (1200 gsm) 0 Deg fabric_OCV Advantex 1322UD (1322 gsm) 0 Deg fabric_PPG-Devold L1200/G50-E07 0 Deg fabric_PPG-Devold L1200/G50-E07/2026 0 Deg fabric_SparPreg UC600, 600 gsm 0 Deg fabric_Taekwang T20C 0 Deg fabric_Vectorply E-LM-1810 0 Deg fabric_Vectorply E-LM-3610 0 Deg fabric_Vectorply E-LT-5500 0 Deg fabric_Vectorply ELT-5500 0 Deg fabric_Zoltek PX3505015T-13 90 deg fabric_8 90 deg fabric_100 Cure / Post Cure_24h/20C, 12h/80 C Cure / Post Cure_24h/20C, 3h/ 65 C \\ 7 MED MED NaN HIGH MED NaN False True False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False False True False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False 8 LOW MED NaN HIGH MED NaN False True False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False False True False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False 9 MED MED NaN HIGH MED NaN False True False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False False True False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False 10 MED MED NaN HIGH MED NaN False True False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False False True False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False 11 MED MED NaN HIGH MED NaN False True False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False False True False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False 12 MED MED NaN HIGH MED NaN False True False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False False True False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False 13 MED MED NaN HIGH MED NaN False True False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False False True False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False 14 MED MED NaN HIGH MED NaN False True False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False False True False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False 15 LOW MED NaN HIGH MED NaN False True False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False False True False False False False False False False False False False False False False False False False False False False False True False False False False False False False False False 16 LOW MED NaN HIGH MED NaN False True False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False True False False False False False False . ",
    "url": "/docs/CSCI%205622%20-%20ML/models/arm/#data",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/arm/#data"
  },"3": {
    "doc": "Association Rule Mining <b>(ARM)</b>",
    "title": "Code",
    "content": "The code is provided in the GitHub repository using the link below: . ARM Code . ",
    "url": "/docs/CSCI%205622%20-%20ML/models/arm/#code",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/arm/#code"
  },"4": {
    "doc": "Association Rule Mining <b>(ARM)</b>",
    "title": "Results",
    "content": "The ARM algorithm was applied to the dataset. For the apriori algorithm, the minimum support was set to 0.25. To find the association rules, the minimum confidence was set to 0.5. The following rules were generated: . antecedents consequents antecedent support consequent support support confidence lift leverage conviction zhangs_metric 0 (Lay-up_[0]2) (Process_Infusion) 0.318 0.743 0.273 0.860 1.157 0.037 1.831 0.199 1 (Lay-up_[0]2) (Max. Stress, MPa_MED) 0.318 0.475 0.268 0.843 1.774 0.117 3.343 0.640 2 (Max. Stress, MPa_MED) (Lay-up_[0]2) 0.475 0.318 0.268 0.564 1.774 0.117 1.564 0.831 3 (Process_Infusion) (Resin Name_EP1) 0.743 0.441 0.441 0.594 1.346 0.113 1.376 1.000 4 (Resin Name_EP1) (Process_Infusion) 0.441 0.743 0.441 1.000 1.346 0.113 inf 0.460 5 (Resin Name_EP1) (Vf, %_MED) 0.441 0.512 0.283 0.643 1.256 0.058 1.367 0.365 6 (Vf, %_MED) (Resin Name_EP1) 0.512 0.441 0.283 0.554 1.256 0.058 1.253 0.418 7 (Resin Name_EP1) (Max. Stress, MPa_MED) 0.441 0.475 0.252 0.571 1.203 0.042 1.225 0.302 8 (Max. Stress, MPa_MED) (Resin Name_EP1) 0.475 0.441 0.252 0.530 1.203 0.042 1.190 0.321 9 (Process_Infusion) (Vf, %_MED) 0.743 0.512 0.438 0.590 1.153 0.058 1.191 0.516 10 (Vf, %_MED) (Process_Infusion) 0.512 0.743 0.438 0.856 1.153 0.058 1.791 0.272 11 (Process_Infusion) (Max. Stress, MPa_MED) 0.743 0.475 0.412 0.555 1.168 0.059 1.179 0.559 12 (Max. Stress, MPa_MED) (Process_Infusion) 0.475 0.743 0.412 0.867 1.168 0.059 1.940 0.274 13 (Max. % Strain_HIGH) (Process_Infusion) 0.315 0.743 0.252 0.800 1.077 0.018 1.286 0.104 14 (Max. Stress, MPa_MED) (Vf, %_MED) 0.475 0.512 0.255 0.536 1.047 0.011 1.052 0.086 15 (Max. % Strain_HIGH) (Max. Stress, MPa_MED) 0.315 0.475 0.273 0.867 1.824 0.123 3.937 0.660 16 (Max. Stress, MPa_MED) (Max. % Strain_HIGH) 0.475 0.315 0.273 0.575 1.824 0.123 1.610 0.861 17 (Process_Infusion, Resin Name_EP1) (Vf, %_MED) 0.441 0.512 0.283 0.643 1.256 0.058 1.367 0.365 18 (Process_Infusion, Vf, %_MED) (Resin Name_EP1) 0.438 0.441 0.283 0.647 1.467 0.090 1.582 0.566 19 (Resin Name_EP1, Vf, %_MED) (Process_Infusion) 0.283 0.743 0.283 1.000 1.346 0.073 inf 0.359 20 (Resin Name_EP1) (Process_Infusion, Vf, %_MED) 0.441 0.438 0.283 0.643 1.467 0.090 1.573 0.569 21 (Vf, %_MED) (Process_Infusion, Resin Name_EP1) 0.512 0.441 0.283 0.554 1.256 0.058 1.253 0.418 22 (Process_Infusion, Resin Name_EP1) (Max. Stress, MPa_MED) 0.441 0.475 0.252 0.571 1.203 0.042 1.225 0.302 23 (Process_Infusion, Max. Stress, MPa_MED) (Resin Name_EP1) 0.412 0.441 0.252 0.611 1.387 0.070 1.439 0.474 24 (Resin Name_EP1, Max. Stress, MPa_MED) (Process_Infusion) 0.252 0.743 0.252 1.000 1.346 0.065 inf 0.344 25 (Resin Name_EP1) (Process_Infusion, Max. Stress, MPa_MED) 0.441 0.412 0.252 0.571 1.387 0.070 1.372 0.499 26 (Max. Stress, MPa_MED) (Process_Infusion, Resin Name_EP1) 0.475 0.441 0.252 0.530 1.203 0.042 1.190 0.321 . So a total of 27 rules were generated. Top 15 rules when sorted by support are as follows: . antecedents consequents antecedent support consequent support support confidence lift leverage conviction zhangs_metric 3 (Process_Infusion) (Resin Name_EP1) 0.743 0.441 0.441 0.594 1.346 0.113 1.376 1.000 4 (Resin Name_EP1) (Process_Infusion) 0.441 0.743 0.441 1.000 1.346 0.113 inf 0.460 10 (Vf, %_MED) (Process_Infusion) 0.512 0.743 0.438 0.856 1.153 0.058 1.791 0.272 9 (Process_Infusion) (Vf, %_MED) 0.743 0.512 0.438 0.590 1.153 0.058 1.191 0.516 11 (Process_Infusion) (Max. Stress, MPa_MED) 0.743 0.475 0.412 0.555 1.168 0.059 1.179 0.559 12 (Max. Stress, MPa_MED) (Process_Infusion) 0.475 0.743 0.412 0.867 1.168 0.059 1.940 0.274 20 (Resin Name_EP1) (Process_Infusion, Vf, %_MED) 0.441 0.438 0.283 0.643 1.467 0.090 1.573 0.569 19 (Resin Name_EP1, Vf, %_MED) (Process_Infusion) 0.283 0.743 0.283 1.000 1.346 0.073 inf 0.359 18 (Process_Infusion, Vf, %_MED) (Resin Name_EP1) 0.438 0.441 0.283 0.647 1.467 0.090 1.582 0.566 17 (Process_Infusion, Resin Name_EP1) (Vf, %_MED) 0.441 0.512 0.283 0.643 1.256 0.058 1.367 0.365 21 (Vf, %_MED) (Process_Infusion, Resin Name_EP1) 0.512 0.441 0.283 0.554 1.256 0.058 1.253 0.418 6 (Vf, %_MED) (Resin Name_EP1) 0.512 0.441 0.283 0.554 1.256 0.058 1.253 0.418 5 (Resin Name_EP1) (Vf, %_MED) 0.441 0.512 0.283 0.643 1.256 0.058 1.367 0.365 15 (Max. % Strain_HIGH) (Max. Stress, MPa_MED) 0.315 0.475 0.273 0.867 1.824 0.123 3.937 0.660 16 (Max. Stress, MPa_MED) (Max. % Strain_HIGH) 0.475 0.315 0.273 0.575 1.824 0.123 1.610 0.861 . Top 15 rules when sorted by confidence are as follows: . antecedents consequents antecedent support consequent support support confidence lift leverage conviction zhangs_metric 24 (Resin Name_EP1, Max. Stress, MPa_MED) (Process_Infusion) 0.252 0.743 0.252 1.000 1.346 0.065 inf 0.344 19 (Resin Name_EP1, Vf, %_MED) (Process_Infusion) 0.283 0.743 0.283 1.000 1.346 0.073 inf 0.359 4 (Resin Name_EP1) (Process_Infusion) 0.441 0.743 0.441 1.000 1.346 0.113 inf 0.460 12 (Max. Stress, MPa_MED) (Process_Infusion) 0.475 0.743 0.412 0.867 1.168 0.059 1.940 0.274 15 (Max. % Strain_HIGH) (Max. Stress, MPa_MED) 0.315 0.475 0.273 0.867 1.824 0.123 3.937 0.660 0 (Lay-up_[0]2) (Process_Infusion) 0.318 0.743 0.273 0.860 1.157 0.037 1.831 0.199 10 (Vf, %_MED) (Process_Infusion) 0.512 0.743 0.438 0.856 1.153 0.058 1.791 0.272 1 (Lay-up_[0]2) (Max. Stress, MPa_MED) 0.318 0.475 0.268 0.843 1.774 0.117 3.343 0.640 13 (Max. % Strain_HIGH) (Process_Infusion) 0.315 0.743 0.252 0.800 1.077 0.018 1.286 0.104 18 (Process_Infusion, Vf, %_MED) (Resin Name_EP1) 0.438 0.441 0.283 0.647 1.467 0.090 1.582 0.566 5 (Resin Name_EP1) (Vf, %_MED) 0.441 0.512 0.283 0.643 1.256 0.058 1.367 0.365 20 (Resin Name_EP1) (Process_Infusion, Vf, %_MED) 0.441 0.438 0.283 0.643 1.467 0.090 1.573 0.569 17 (Process_Infusion, Resin Name_EP1) (Vf, %_MED) 0.441 0.512 0.283 0.643 1.256 0.058 1.367 0.365 23 (Process_Infusion, Max. Stress, MPa_MED) (Resin Name_EP1) 0.412 0.441 0.252 0.611 1.387 0.070 1.439 0.474 3 (Process_Infusion) (Resin Name_EP1) 0.743 0.441 0.441 0.594 1.346 0.113 1.376 1.000 . Top 15 rules when sorted by lift are as follows: . antecedents consequents antecedent support consequent support support confidence lift leverage conviction zhangs_metric 16 (Max. Stress, MPa_MED) (Max. % Strain_HIGH) 0.475 0.315 0.273 0.575 1.824 0.123 1.610 0.861 15 (Max. % Strain_HIGH) (Max. Stress, MPa_MED) 0.315 0.475 0.273 0.867 1.824 0.123 3.937 0.660 1 (Lay-up_[0]2) (Max. Stress, MPa_MED) 0.318 0.475 0.268 0.843 1.774 0.117 3.343 0.640 2 (Max. Stress, MPa_MED) (Lay-up_[0]2) 0.475 0.318 0.268 0.564 1.774 0.117 1.564 0.831 20 (Resin Name_EP1) (Process_Infusion, Vf, %_MED) 0.441 0.438 0.283 0.643 1.467 0.090 1.573 0.569 18 (Process_Infusion, Vf, %_MED) (Resin Name_EP1) 0.438 0.441 0.283 0.647 1.467 0.090 1.582 0.566 23 (Process_Infusion, Max. Stress, MPa_MED) (Resin Name_EP1) 0.412 0.441 0.252 0.611 1.387 0.070 1.439 0.474 25 (Resin Name_EP1) (Process_Infusion, Max. Stress, MPa_MED) 0.441 0.412 0.252 0.571 1.387 0.070 1.372 0.499 3 (Process_Infusion) (Resin Name_EP1) 0.743 0.441 0.441 0.594 1.346 0.113 1.376 1.000 4 (Resin Name_EP1) (Process_Infusion) 0.441 0.743 0.441 1.000 1.346 0.113 inf 0.460 19 (Resin Name_EP1, Vf, %_MED) (Process_Infusion) 0.283 0.743 0.283 1.000 1.346 0.073 inf 0.359 24 (Resin Name_EP1, Max. Stress, MPa_MED) (Process_Infusion) 0.252 0.743 0.252 1.000 1.346 0.065 inf 0.344 5 (Resin Name_EP1) (Vf, %_MED) 0.441 0.512 0.283 0.643 1.256 0.058 1.367 0.365 17 (Process_Infusion, Resin Name_EP1) (Vf, %_MED) 0.441 0.512 0.283 0.643 1.256 0.058 1.367 0.365 21 (Vf, %_MED) (Process_Infusion, Resin Name_EP1) 0.512 0.441 0.283 0.554 1.256 0.058 1.253 0.418 . ",
    "url": "/docs/CSCI%205622%20-%20ML/models/arm/#results",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/arm/#results"
  },"5": {
    "doc": "Association Rule Mining <b>(ARM)</b>",
    "title": "Conclusion",
    "content": "There are some obvious results from the generated rules. For example, the max stresses and max strains are highly correlated. Similarly, the process of infusion is highly correlated with EP1 material. This is expected as EP1 is made with an infusion process. Some interesting observations are: . | The max stress is highly correlated with the infusion process. | EP1 material is highly correlated with the infusion process, performs relatively well with stress and strain values. | Med Vf% is highly correlated with the infusion process and EP1 material. This is a good observation in order to optimize the fiber volume fraction in the composite material. | . ",
    "url": "/docs/CSCI%205622%20-%20ML/models/arm/#conclusion",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/arm/#conclusion"
  },"6": {
    "doc": "Association Rule Mining <b>(ARM)</b>",
    "title": "Association Rule Mining <b>(ARM)</b>",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/arm/",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/arm/"
  },"7": {
    "doc": "Clustering",
    "title": "Clustering",
    "content": ". | Overview | Data | Code | Results . | K-Means Clustering | Hierarchical Clustering | DBSCAN Clustering | . | . ",
    "url": "/docs/CSCI%205622%20-%20ML/models/clustering/",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/clustering/"
  },"8": {
    "doc": "Clustering",
    "title": "Overview",
    "content": "Clustering is an unsupervised machine learning technique used to group similar data points together. The goal of clustering is to identify patterns and structures in the data that are not explicitly labeled. Clustering algorithms partition the data into groups based on the similarity of data points, with the objective of maximizing the intra-cluster similarity and minimizing the inter-cluster similarity. Some key concepts in clustering include: . | Centroid: The center of a cluster, which is calculated as the mean of all data points in the cluster. | Distance Metric: A measure of similarity between data points, which is used to determine which points belong to the same cluster. | . In this project, clustering will be used to identify materials of wind turbine blades with similar properties. The clustering algorithm will group together items with similar material composition, and performance characteristics. This information can be used to identify important features of materials to consider and will help drop the less important features. There are several clustering algorithms available, each with its own strengths and weaknesses. Some common clustering algorithms used in this project include: . | K-Means: A partitioning algorithm that divides the data into K clusters by iteratively assigning data points to the nearest cluster centroid and updating the centroids based on the mean of the data points in each cluster. | Hierarchical Clustering: A hierarchical algorithm that builds a tree of clusters by iteratively merging or splitting clusters based on the similarity of data points. | DBSCAN (Density-Based Spatial Clustering of Applications with Noise): A density-based algorithm that groups together data points that are closely packed and separates outliers as noise. | . References: [1]: https://keytodatascience.com/k-means-clustering-algorithm/ [2]: https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68 [3]: https://www.geeksforgeeks.org/dbscan-clustering-in-ml-density-based-clustering/ . ",
    "url": "/docs/CSCI%205622%20-%20ML/models/clustering/#overview",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/clustering/#overview"
  },"9": {
    "doc": "Clustering",
    "title": "Data",
    "content": "The data is prepped in a similar fashion to the PCA model. Please refer to the PCA page for more information. The n_components = 3 is used to reduce the dimensionality of the dataset to 3 principal components. PCA Data n=3 (first 10 rows): pc1 pc2 pc3 Resin Type 0 -2.189 -0.355 0.009 Epoxy 1 -2.254 -0.525 0.004 Epoxy 2 -2.139 -0.201 0.014 Epoxy 3 -2.123 -0.092 0.018 Epoxy 4 -2.119 -0.072 0.013 Epoxy 5 -2.194 -0.174 0.014 Epoxy 6 -2.079 0.144 0.022 Epoxy 7 -2.238 -0.220 0.014 Epoxy 8 -2.233 -0.495 0.004 Epoxy 9 -2.238 -0.443 -0.000 Epoxy Cumulative Explained Variance Ratio: [0.42561841 0.66286118 0.82963867] . So the first 3 principal components explain 82.96% of the variance in the data. This data will be used to perform all clustering algorithms. ",
    "url": "/docs/CSCI%205622%20-%20ML/models/clustering/#data",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/clustering/#data"
  },"10": {
    "doc": "Clustering",
    "title": "Code",
    "content": "The code is provided in the GitHub repository using the link below: . Clustering Code . ",
    "url": "/docs/CSCI%205622%20-%20ML/models/clustering/#code",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/clustering/#code"
  },"11": {
    "doc": "Clustering",
    "title": "Results",
    "content": "K-Means Clustering . The K-Means algorithm was applied to the dataset with n_clusters=[2,3,4]. Here is the plot of the clusters: . The plots show the original data points colored by their true labels and the centroids of the clusters for each value of k. By comparing these plots, you can see how the clustering changes with different values of k. k=2: This clustering seems to separate the data into two distinct groups, but it might not capture the full complexity of the data. k=3: This clustering appears to identify three clusters that might better represent the underlying structure of the data. k=4: This clustering results in four clusters, but it might be overfitting the data, as the clusters seem to fit some outliers. In this case, k=3 seems to provide a good balance between capturing the structure of the data and avoiding overfitting. Hierarchical Clustering . The Hierarchical Clustering algorithm was applied to the dataset. Here is the dendrogram of the clusters: . The dendrogram shows the hierarchical structure of the clusters, with the data points at the bottom and the clusters at the top. We can see that in this case, the small cluster (orange) is not considered an outlier, but rather a separate cluster. This might be due to the hierarchical nature of the algorithm, which allows for more flexibility in defining clusters. Otherwise, the results match the K-Means clustering with k=3 without the need to specify the number of clusters. DBSCAN Clustering . The DBSCAN algorithm was applied to the dataset with eps=0.5 and min_samples=10. Here is the plot of the clusters: . The plot shows the original data points colored by their true labels and the clusters identified by DBSCAN. The algorithm seems to identify three main clusters and some outliers. The clusters are well-separated and capture the structure of the data well. The outliers are labeled as noise in black, which is a useful feature of this method. DBSCAN was also applied with eps=0.8 and min_samples=10. Here is the plot of the clusters: . We still notice three main clusters, but the outliers are now included in the clusters. This might be due to the larger value of eps, which allows for more points to be considered part of the same cluster. For this case, DBSCAN with eps=0.5 seems to provide a good balance between capturing the structure of the data and avoiding overfitting. To compare the results of the clustering algorithms, we can use metrics such as the silhouette score, which measures the quality of the clusters. The silhouette score ranges from -1 to 1, with higher values indicating better clustering. The silhouette score for each algorithm is as follows: . K-means (k=3) Silhouette Score: 0.567 Hierarchical Clustering Silhouette Score: 0.667 DBSCAN Silhouette Score (eps=0.5): 0.532 . So, in this case, Hierarchical Clustering seems to provide the best clustering results based on the silhouette score. ",
    "url": "/docs/CSCI%205622%20-%20ML/models/clustering/#results",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/clustering/#results"
  },"12": {
    "doc": "Conclusion",
    "title": "Conclusion",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/conclusion/",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/conclusion/"
  },"13": {
    "doc": "Data Preparation/EDA",
    "title": "Data Preparation/EDA",
    "content": ". | Introduction | Data Gathering - NewsAPI | Data Cleaning and Visualization - NewsAPI | Data Cleaning - OptiDAT Database | Prelim Exploratory Data Analysis - OptiDAT Database | . ",
    "url": "/docs/CSCI%205622%20-%20ML/data_prep/",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/data_prep/"
  },"14": {
    "doc": "Data Preparation/EDA",
    "title": "Introduction",
    "content": "The main data used for analysis is obtained from the OptiDAT Database. This data contains information about the mechanical properties of different composite materials used in wind turbine blades. The data is in the form of an Excel file. Furthermore, the NewsAPI is used to scrape news articles about the wind energy sector. This data is used to analyze the current trends and show the growing importance of recyclability in the wind energy sector. ",
    "url": "/docs/CSCI%205622%20-%20ML/data_prep/#introduction",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/data_prep/#introduction"
  },"15": {
    "doc": "Data Preparation/EDA",
    "title": "Data Gathering - NewsAPI",
    "content": "The NewsAPI is used to gather news articles about the wind energy sector and analyze the trends over the last couple decades. Here is the code that will be used to gather the articles: . import requests import re import pandas as pd import matplotlib.pyplot as plt from datetime import datetime # Connecting to NewsAPI and getting data end = 'https://newsapi.org/v2/everything' all_jsons = [] for page in range(1, 6): URL_id = {'apiKey' : 'de147731dffe48aa9effd3380691833b', 'q' : 'wind energy AND (recycle OR recycling OR recyclable OR recyclability)', 'from' : '1990-01-01', 'to' : datetime.now().strftime('%Y-%m-%d'), 'language' : 'en', 'pageSize' : 100, 'page' : page } response = requests.get(end, URL_id) jsontxt = response.json() all_jsons.append(jsontxt) . Data from all 5 pages are gathered in a list that will be used to create a DataFrame of articles. ",
    "url": "/docs/CSCI%205622%20-%20ML/data_prep/#data-gathering---newsapi",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/data_prep/#data-gathering---newsapi"
  },"16": {
    "doc": "Data Preparation/EDA",
    "title": "Data Cleaning and Visualization - NewsAPI",
    "content": "# Create a DataFrame from the articles df = pd.DataFrame(all_articles) # Convert the publishedAt column to datetime df['publishedAt'] = pd.to_datetime(df['publishedAt']) # Extract the year from the publishedAt column df['year'] = df['publishedAt'].dt.year # Group by year and count the number of articles trends = df.groupby('year').size().reset_index(name='article_count') # Plot the trends fig, ax = plt.subplots() ax.plot(trends['year'], trends['article_count'], marker='o') ax.set_title('Trends in Wind Energy and Recyclability Articles (1990-Present)') ax.set_xlabel('Year') ax.set_ylabel('Number of Articles') . This idea did not work as expected due to the restrictions on the free version of the NewsAPI platform. The oldest data available was from 8-17-2024 so only one month. In the next iteration, the data will be gathered from a different source so that the trends can be analyzed. Regardless, the data from the OptiDAT database has been gathered, cleaned and run through a preliminary analysis as shown below. ",
    "url": "/docs/CSCI%205622%20-%20ML/data_prep/#data-cleaning-and-visualization---newsapi",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/data_prep/#data-cleaning-and-visualization---newsapi"
  },"17": {
    "doc": "Data Preparation/EDA",
    "title": "Data Cleaning - OptiDAT Database",
    "content": "The OptiDAT data is in the form of an Excel file. The main data is in the OPTIMAT Database sheetname. It consists of 93 columns and 3435 rows. The columns contain information about the material properties, such as density, tensile strength, compressive strength, etc. The rows contain data about different composite materials that were tested as part of the study. Here is a snippet of the raw data: . Using python, the raw data is loaded into a pandas dataframe. import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns file_name = './data/Optidat_dataset.xls' loc = os.getcwd() file_loc = os.path.join(loc,file_name) raw_data = pd.read_excel(file_loc, sheet_name='OPTIMAT Database', header=4) print('Raw Data Info: \\n') print(raw_data.info()) . Here is a look at the data summary. Raw Data Info: &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 3435 entries, 0 to 3434 Data columns (total 93 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Optidat version: February 10, 2011 0 non-null float64 1 optiDAT nr. 3435 non-null int64 2 Name 3435 non-null object 3 Plate 3435 non-null object 4 Fibre Volume Fraction 3031 non-null float64 5 Lab 3435 non-null object 6 Laminate 3435 non-null object 7 Cut angle 3435 non-null object 8 Manufacturer 3435 non-null object 9 Geometry 3435 non-null object 10 Material 3435 non-null object 11 taverage 3232 non-null object 12 wmax 2084 non-null float64 13 wmin 2814 non-null float64 14 waverage 3181 non-null float64 15 area 3181 non-null float64 16 Lnominal 3435 non-null object 17 Lmeasured 836 non-null float64 18 Lgauge 2105 non-null float64 19 Radius (waist) 92 non-null float64 20 TG and phase 3435 non-null object 21 Phase 3435 non-null object 22 start date 3253 non-null datetime64[ns] 23 end date 3143 non-null datetime64[ns] 24 Test type 3319 non-null object 25 R-value1 2003 non-null object 26 Fmax 3256 non-null float64 27 Ferror 0 non-null float64 28 Fstatic 1788 non-null float64 29 Ffatigue 2045 non-null float64 30 Fmax, 90° 122 non-null float64 31 epsmax 1871 non-null float64 32 epsstatic 992 non-null float64 33 epsfatigue 977 non-null float64 34 eps90° 112 non-null float64 35 eps45° 136 non-null float64 36 Nu 286 non-null float64 37 smax 3035 non-null float64 38 smax,static 1627 non-null float64 39 smax, fatigue 1976 non-null float64 40 shear strain 95 non-null float64 41 shear strength 115 non-null float64 42 shear modulus (near other moduli) 127 non-null float64 43 Torquemax 1311 non-null float64 44 Torquefatigue 50 non-null float64 45 Torquestatic 51 non-null float64 46 Ncycles 3300 non-null object 47 level 1484 non-null object 48 levelcalc 1984 non-null object 49 Nspectrum 100 non-null float64 50 failure mode 1560 non-null object 51 runout 51 non-null object 52 R-value2 142 non-null float64 53 Fmax2 221 non-null float64 54 epsmax2 87 non-null float64 55 smax2 142 non-null float64 56 Ncycles2 141 non-null float64 57 ratedisplacement 1516 non-null object 58 f 2038 non-null object 59 f2 142 non-null float64 60 Eit 1837 non-null float64 61 Eic 1416 non-null object 62 Eft 398 non-null float64 63 Efc 168 non-null float64 64 Elur,front 9 non-null object 65 Elur, back 9 non-null object 66 eps_max_t 1096 non-null float64 67 eps_max_c 700 non-null float64 68 Machine 3345 non-null object 69 control 3125 non-null object 70 grip 3108 non-null object 71 ABG 2574 non-null object 72 Temp. 718 non-null object 73 Temp. control 1779 non-null object 74 Environment 3435 non-null object 75 Reference document 2552 non-null object 76 Remarks 1847 non-null object 77 Invalid 165 non-null object 78 Bending 167 non-null object 79 Buckling 55 non-null object 80 Overheating 28 non-null object 81 Tab failure 262 non-null object 82 Delaminated 22 non-null object 83 Incomplete measurement data 207 non-null object 84 Strain from E 29 non-null object 85 LUR 217 non-null object 86 TEC 50 non-null float64 87 data delivered under name 803 non-null object 88 Repair characteristics 167 non-null object 89 Strain measurement equipment (long) 2209 non-null object 90 Strain measurement equipment (short) 2225 non-null object 91 Grip pressure 1876 non-null object 92 time per test 2909 non-null float64 dtypes: datetime64[ns](2), float64(43), int64(1), object(47) . Now, the data is cleaned by removing columns that are not needed for analysis. The columns are removed as follows: . # Remove whitespace leading and trailing column names data.rename(columns=lambda x: x.strip(), inplace=True) column_list = ['Name', 'Fibre Volume Fraction', 'Material','Test type', 'smax,static','smax, fatigue','epsstatic','epsfatigue', 'Ncycles','Eit','Eic', 'Invalid','Incomplete measurement data'] data = data.filter(column_list) data.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 3435 entries, 0 to 3434 Data columns (total 13 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Name 3435 non-null object 1 Fibre Volume Fraction 3031 non-null float64 2 Material 3435 non-null object 3 Test type 3319 non-null object 4 smax,static 1627 non-null float64 5 smax, fatigue 1976 non-null float64 6 epsstatic 992 non-null float64 7 epsfatigue 977 non-null float64 8 Ncycles 3300 non-null object 9 Eit 1837 non-null float64 10 Eic 1416 non-null object 11 Invalid 165 non-null object 12 Incomplete measurement data 207 non-null object dtypes: float64(6), object(7) . Two important columns to easily remove data is by looking at the Invalid data column and the Incomplete measurement data column. If the data is invalid or incomplete, it is removed from the dataset. Also, rows that do not have a designated test type are removed. # Remove rows for data that is Invalid data = data[data['Invalid'].isna()] print('Total Invalid Data:',data['Invalid'].notna().sum()) # Remove rows for data that is Incomplete data = data[data['Incomplete measurement data'].isna()] print('Total Incomplete measurement Data:',data['Incomplete measurement data'].notna().sum()) # Drop the two columns data.drop(['Invalid','Incomplete measurement data'],axis=1,inplace=True) # Remove rows that do not have a test type data = data[data['Test type'].notna()] data.info() . Here is the current state of columns and data types: . &lt;class 'pandas.core.frame.DataFrame'&gt; Index: 3068 entries, 0 to 3433 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Name 3068 non-null object 1 Fibre Volume Fraction 2729 non-null float64 2 Material 3068 non-null object 3 Test type 3068 non-null object 4 smax,static 1518 non-null float64 5 smax, fatigue 1752 non-null float64 6 epsstatic 961 non-null float64 7 epsfatigue 927 non-null float64 8 Ncycles 2983 non-null object 9 Eit 1759 non-null float64 10 Eic 1336 non-null object dtypes: float64(6), object(5) . Now we correct the data types for the columns. The Name column is temporarily left as an object dtype. More on this will be discussed later. The Test type and Material columns are also converted to a category data type. Finally, the Ncycles and Eic columns are to be converted to a float data type. # Change data types data['Material'] = data['Material'].astype('category') data['Test type'] = data['Test type'].astype('category') data['Ncycles'] = data['Ncycles'].astype('float64') data['Eic'] = data['Eic'].astype('float64') . The dtype conversion of data['Eic'] is showing an error because of a string value in the column. First we need to find all the string values in the column. # Eic should both be all float or int for i,u in enumerate(data['Eic']): if type(u) == str: print(i, u) . Row 325 has an error for Eic data; value has whitespaces ' '. This is corrected by converting the string to a np.nan value. data['Eic'] = np.where(data['Eic'] == ' ', np.nan, data['Eic']) # Try changing dtype again data['Eic'] = data['Eic'].astype('float64') # check with data.info() or data.describe() data.info() . &lt;class 'pandas.core.frame.DataFrame'&gt; Index: 3068 entries, 0 to 3433 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Name 3068 non-null object 1 Fibre Volume Fraction 2729 non-null float64 2 Material 3068 non-null category 3 Test type 3068 non-null category 4 smax,static 1518 non-null float64 5 smax, fatigue 1752 non-null float64 6 epsstatic 961 non-null float64 7 epsfatigue 927 non-null float64 8 Ncycles 2983 non-null float64 9 Eit 1759 non-null float64 10 Eic 1335 non-null float64 dtypes: category(2), float64(8), object(1) . Fibre Volume Fraction smax,static smax, fatigue epsstatic epsfatigue Ncycles Eit Eic count 2729.000 1518.000 1752.000 961.000 927.000 2983.000 1759.000 1335.000 mean 45.004 113.787 127.534 0.672 0.454 345061.136 26.331 29.376 std 18.919 476.216 221.863 1.872 0.698 1514505.719 9.902 8.981 min 0.000 -5274.270 -511.466 -3.846 -1.666 0.000 0.778 10.750 25% 50.868 -387.185 42.627 -1.332 0.173 1.000 14.670 25.800 50% 52.480 102.466 181.912 1.300 0.608 2452.000 27.530 28.619 75% 53.730 505.058 264.490 2.121 0.910 91196.000 34.600 38.200 max 56.660 1156.504 761.461 8.080 1.970 39393907.000 49.300 57.160 . ",
    "url": "/docs/CSCI%205622%20-%20ML/data_prep/#data-cleaning---optidat-database",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/data_prep/#data-cleaning---optidat-database"
  },"18": {
    "doc": "Data Preparation/EDA",
    "title": "Prelim Exploratory Data Analysis - OptiDAT Database",
    "content": "The data looks relatively clean and ready for some exploratory data analysis with visualizations. Visualizing the data can be helpful to clean the data further. Looking at the raw data, the test type column has a lot of unique values. It might be helpful to visualize the top 10 test types. # Start by visualizing all Test Types # tt = data['Test type'].unique() # Test type pie chart (Top 10) tt_counts = data['Test type'].value_counts() print(tt_counts) if plot: fig, ax = plt.subplots() fig.suptitle('Top 10 test types and total replicates', size=20) ax.pie(tt_counts[:10], autopct=lambda p: '{:.0f}'.format(p * sum(tt_counts[:10]) / 100), labels = tt_counts.index[:10]) . tt_counts shows that the test type column has 83 unique values. The top 10 test types are shown in the pie chart. To narrow the analysis and only include the important test types, top 3 test types are selected. The data is filtered to only include the top 3 test types - STT (Static Tensile), STC (Static Compression) and CA (Constant Amplitude Fatigue). # Only keep CA, STT, and STC tt_list = ['STT','STC','CA'] data = data[data['Test type'].isin(tt_list)] # Plot the top 3 test types if plot: fig, ax = plt.subplots() fig.suptitle('Top 3 test types and total replicates', size=20) ax.pie(tt_counts[:3], autopct=lambda p: '{:.0f}'.format(p * sum(tt_counts[:3]) / 100), labels = tt_counts[:3].index) plt.savefig('./imgs/Pie_Chart_T3.png') . The preliminary data can be further grouped and analyzed by the major differtiator i.e. Test type. # Group data by Test Type for visualizations tt_group = data.groupby('Test type', observed=True) tensile_data = tt_group.get_group('STT') compression_data = tt_group.get_group('STC') fatigue_data = tt_group.get_group('CA') # data head for tensile tests tensile_data.head(10) . Here is the data for the initial 10 tensile tests. Similar data are grouped for compression and fatigue (CA) tests. Name Fibre Volume Fraction Material Test type smax,static smax, fatigue epsstatic epsfatigue Ncycles Eit Eic 5 GEV201_D0100_0016 NaN GE1 STT 812.248 NaN 2.104 NaN 1.000 44.578 NaN 9 GEV201_D0100_0031 NaN GE1 STT 859.098 NaN 2.148 NaN 1.000 44.911 NaN 10 GEV201_R0100_0001 NaN GE1 STT 879.473 NaN NaN NaN 1.000 39.430 NaN 14 GEV201_R0100_0011 NaN GE1 STT 841.836 NaN NaN NaN 1.000 39.523 NaN 20 GEV201_R0100_0026 NaN GE1 STT 751.744 NaN -1.960 NaN 1.000 39.190 NaN 26 GEV202_D0100_0001 NaN GE1 STT 712.281 NaN NaN NaN 1.000 NaN NaN 28 GEV202_D0100_0006 NaN GE1 STT 688.875 NaN NaN NaN 1.000 NaN NaN 38 GEV202_R0100_0006 NaN GE1 STT 689.609 NaN NaN NaN 1.000 NaN NaN 40 GEV202_R0100_0011 NaN GE1 STT 675.225 NaN NaN NaN 1.000 NaN NaN 59 GEV203_R0100_0001 NaN GE1 STT 483.126 NaN NaN NaN 1.000 NaN NaN . np.nan values in certain columns are expected as fatigue related test data will not be available for tensile and compression tests and vice versa. Some easy visualizations can be done to understand the data better. For example, a box or violin plot to show the distribution of the data. But first, the stress data for tensile must be cleaned to remove invalid values (negative values) and outliers. # Cleaning tensile data tensile_data = tensile_data[tensile_data['smax,static']&gt;0] tensile_data.dropna(subset=['smax,static'], inplace=True) # Visualize the data if plot: fig, ax = plt.subplots() sns.violinplot(data=tensile_data, y='smax,static', ax=ax) ax.set_xlabel('Probability Distribution Function (width ~ frequency)', size=10) ax.set_ylabel(r'Max Stress - $\\sigma_{max}$ (MPa) ', size=10) ax.set_title('Tensile Test Max Stress Distribution', size=16) plt.savefig('./imgs/tensile_max_stress_dist.png') . Similarly the data for compression and fatigue tests can be cleaned and visualized. if plot: fig, ax = plt.subplots() sns.violinplot(data=compression_data, y='smax,static', ax=ax) ax.set_xlabel('Probability Distribution Function (width ~ frequency)', size=10) ax.set_ylabel(r'Max Stress - $\\sigma_{max}$ (MPa) ', size=10) ax.set_title('Compression Test Max Stress Distribution', size=16) plt.savefig('./imgs/compression_max_stress_dist_not-clean.png') . However, we see that the compression data has some values that are positive. This is not expected for compression tests. The data is cleaned by removing the positive values. # Cleaning compression data compression_data = compression_data[compression_data['smax,static']&lt;0] compression_data.dropna(subset=['smax,static'], inplace=True) if plot: fig, ax = plt.subplots() sns.violinplot(data=compression_data, y='smax,static', ax=ax) ax.set_xlabel('Probability Distribution Function (width ~ frequency)', size=10) ax.set_ylabel(r'Max Stress - $\\sigma_{max}$ (MPa) ', size=10) ax.set_title('Compression Test Max Stress Distribution', size=16) plt.savefig('./imgs/compression_max_stress_dist.png') . Finally, the fatigue max stress data smax, fatigue is plotted. if plot: fig, ax = plt.subplots() sns.violinplot(data=fatigue_data, y='smax, fatigue', ax=ax) ax.set_xlabel('Probability Distribution Function (width ~ frequency)', size=10) ax.set_ylabel(r'Max Stress (Fatigue) - $\\sigma_{max}$ (MPa) ', size=10) ax.set_title('Fatigue Test Max Stress Distribution', size=16) plt.savefig('./imgs/fatigue_max_stress_dist.png') . There is more to the fatigue data than just max stress. The values can be negative if the ratio of min to max stress is not 1. The ratio data is available in the R-value1 column and will be utilized in the analysis later. Now, the fatigue Ncycles data is visualized. Generally, this is done in a log plot. # Plot Ncycles as a function of smax, fatigue if plot: fig, ax = plt.subplots() sns.scatterplot(data=fatigue_data, x='Ncycles', y='smax, fatigue', ax=ax,) ax.set_xscale('log') ax.set_xlabel('Number of cycles till failure', size=10) ax.set_ylabel(r'Max Stress (Fatigue) - $\\sigma_{max}$ (MPa) ', size=10) ax.set_title('S-N Curve for Fatigue data', size=16) plt.savefig('./imgs/fatigue_SN-curve.png') . There might be a relation to the material so the same plot is done with a hue for the material. # Same plot, Hue for material if plot: fig, ax = plt.subplots() sns.scatterplot(data=fatigue_data, x='Ncycles', y='smax, fatigue', ax=ax, hue='Material') ax.set_xscale('log') ax.set_xlabel('Number of cycles till failure', size=10) ax.set_ylabel(r'Max Stress (Fatigue) - $\\sigma_{max}$ (MPa) ', size=10) ax.set_title('S-N Curve for Fatigue data, Hue-Material', size=16) plt.savefig('./imgs/fatigue_SN-curve_Material-hue.png') . Moving back to the tensile data, the tensile modulus data Eit is visualized as a function of the Fiber Volume Fraction in the material. # Check relation between Fiber Volume and Modulus for tensile data if plot: fig, ax = plt.subplots() sns.scatterplot(data=tensile_data, x='Fibre Volume Fraction', y='Eit', ax=ax) ax.set_xlabel('Fibre Volume Fraction (%)', size=10) ax.set_ylabel(r'Elastic Modulus $E_t$ (MPa) ', size=10) ax.set_title('Modulus vs FVF', size=16) plt.savefig('./imgs/E-vs-FVF.png') . Looks like there are two distinct groups in the data. This will be investigated later in the study. All the plots are available in the Introduction section of the project as per the guidelines. ",
    "url": "/docs/CSCI%205622%20-%20ML/data_prep/#prelim-exploratory-data-analysis---optidat-database",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/data_prep/#prelim-exploratory-data-analysis---optidat-database"
  },"19": {
    "doc": "Decision Trees <b>(DT)</b>",
    "title": "Decision Trees (DT)",
    "content": ". | Overview | Data | Code | Results | . ",
    "url": "/docs/CSCI%205622%20-%20ML/models/dt/#decision-trees-dt",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/dt/#decision-trees-dt"
  },"20": {
    "doc": "Decision Trees <b>(DT)</b>",
    "title": "Overview",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/dt/#overview",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/dt/#overview"
  },"21": {
    "doc": "Decision Trees <b>(DT)</b>",
    "title": "Data",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/dt/#data",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/dt/#data"
  },"22": {
    "doc": "Decision Trees <b>(DT)</b>",
    "title": "Code",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/dt/#code",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/dt/#code"
  },"23": {
    "doc": "Decision Trees <b>(DT)</b>",
    "title": "Results",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/dt/#results",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/dt/#results"
  },"24": {
    "doc": "Decision Trees <b>(DT)</b>",
    "title": "Decision Trees <b>(DT)</b>",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/dt/",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/dt/"
  },"25": {
    "doc": "CSCI 5622 - ML",
    "title": "CSCI 5622 - Machine Learning",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/index-CSCI/#csci-5622---machine-learning",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/index-CSCI/#csci-5622---machine-learning"
  },"26": {
    "doc": "CSCI 5622 - ML",
    "title": "CSCI 5622 - ML",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/index-CSCI/",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/index-CSCI/"
  },"27": {
    "doc": "Home",
    "title": "Welcome to Hemant’s Personal Website",
    "content": ". ",
    "url": "/#welcome-to-hemants-personal-website",
    
    "relUrl": "/#welcome-to-hemants-personal-website"
  },"28": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"29": {
    "doc": "Models/Methods",
    "title": "Model/Methods",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/index-models/#modelmethods",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/index-models/#modelmethods"
  },"30": {
    "doc": "Models/Methods",
    "title": "Models/Methods",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/index-models/",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/index-models/"
  },"31": {
    "doc": "Introduction",
    "title": "Introduction",
    "content": ". | Wind Energy . | Raw Data Image | Cleaned Data Image | Visualization Images | . | . ",
    "url": "/docs/CSCI%205622%20-%20ML/introduction/",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/introduction/"
  },"32": {
    "doc": "Introduction",
    "title": "Wind Energy",
    "content": "Wind energy is a key component in the mitigation strategy for climate change. It has grown into a significant sector in the global renewable energy landscape. However, the growth has not been fast enough to meet the climate change targets. The wind energy sector is facing challenges in terms of cost, reliability, efficiency and more recently, recyclability. Wind blades are one of the most expensive components of the turbine as well as the most difficult to recycle. Recyclable wind turbines will help reduce cost by enabling the reuse of expensive materials while reducing waste. This will help wind energy become more competitive in the renewable energy space. Wind Turbines in Landfill as the materials cannot be recycled . This project will aim to address the recyclability of wind blades. The project will focus on the development of a machine learning model to analyze characteristics of current renewable, recyclable composite materials, and compare their properties to that of traditional materials (epoxy and fiberglass). By analyzing the results from current materials, the model will be able to predict which characteristics are most important for recyclable materials. In this project, two datasets will be used. The first dataset is the OptiDAT dataset which contains material properties of wind turbine blades. The second dataset is the SNL/DOE/MSU dataset which also has recent material properties of wind turbine blades. The datasets will be used to train the machine learning model to predict the most important characteristics of recyclable materials. There is a need for new technologies and innovations to address these challenges and make wind energy more competitive in the renewable energy space. Raw Data Image . Cleaned Data Image . Visualization Images . All the images below are visualizations of the data. The code is available in the date_prep tab as well as the process of data cleaning and visualization. Please refer to that tab for more information. Pie chart shows the top 10 test types in the whole dataset . Pie chart shows the top 3 most important test types in the whole dataset . Violin plot showing the distribution of max stress in tensile data for all materials . Violin plot of max stress in compression data. This showed some positive values which needed to be removed . Violin plot of max stress in compression data after removing positive values . Side by side comparison showing the raw and clean data . Violin plot of max stress in fatigue data . Scatter plot showing SN curve relation for fatigue tests . Scatter plot showing the S-N curve for fatigue tests with material hue . Scatter plot showing the relationship between tensile modulus and fiber volume fraction. The plot shows two distinct groups which will be investigated later. Raw data is available on the GitHub repository. OptiDAT dataset . Code used is also available on the GitHub repository. Data Prep Code Prelim NewsAPI code . Images are available in the assets/imgs folder. Images and Plots . References: [1]: https://www.texasmonthly.com/news-politics/sweetwater-wind-turbine-blades-dump/ [2]: Wiser, R.; Bolinger, M.; Hoen, B. Land-Based Wind Market Report: 2022 Edition, Department of Energey Report, 2022. [3]: Komusanac, I.; Brindley, G.; Fraile, D.; Ramirez, L. Wind Energy in Europe: 2021 Statistics and the Outlook for 2022-2026, WindEurope Report, 2022. ",
    "url": "/docs/CSCI%205622%20-%20ML/introduction/#wind-energy",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/introduction/#wind-energy"
  },"33": {
    "doc": "Naive Bayes <b>(NB)</b>",
    "title": "Naive Bayes (NB)",
    "content": ". | Overview | Data | Code | Results | . ",
    "url": "/docs/CSCI%205622%20-%20ML/models/nb/#naive-bayes-nb",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/nb/#naive-bayes-nb"
  },"34": {
    "doc": "Naive Bayes <b>(NB)</b>",
    "title": "Overview",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/nb/#overview",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/nb/#overview"
  },"35": {
    "doc": "Naive Bayes <b>(NB)</b>",
    "title": "Data",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/nb/#data",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/nb/#data"
  },"36": {
    "doc": "Naive Bayes <b>(NB)</b>",
    "title": "Code",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/nb/#code",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/nb/#code"
  },"37": {
    "doc": "Naive Bayes <b>(NB)</b>",
    "title": "Results",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/nb/#results",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/nb/#results"
  },"38": {
    "doc": "Naive Bayes <b>(NB)</b>",
    "title": "Naive Bayes <b>(NB)</b>",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/nb/",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/nb/"
  },"39": {
    "doc": "Principal Component Analysis <b>(PCA)</b>",
    "title": "Principal Component Analysis (PCA)",
    "content": ". | Overview | explain PCA and distance metrics . | Data | Code | Results . | n_components = 2 | n_components = 3 | n_components = 4 | n_components = 5 | . | . | . ",
    "url": "/docs/CSCI%205622%20-%20ML/models/pca/#principal-component-analysis-pca",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/pca/#principal-component-analysis-pca"
  },"40": {
    "doc": "Principal Component Analysis <b>(PCA)</b>",
    "title": "Overview",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/pca/#overview",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/pca/#overview"
  },"41": {
    "doc": "Principal Component Analysis <b>(PCA)</b>",
    "title": "explain PCA and distance metrics",
    "content": "PCA is a dimensionality reduction technique that is widely used in machine learning and data analysis. It is used to reduce the number of features in a dataset while retaining as much information as possible. PCA works by transforming the original features into a new set of orthogonal features called principal components. These components are ordered by the amount of variance they explain in the data. The first principal component explains the most variance, the second principal component explains the second most variance, and so on. By selecting only the top principal components, you can reduce the dimensionality of the dataset while retaining most of the information. Scatter plot showing Principal components V1 and V2 . Scatter plot showing a transformed axis using the orthogonal principal component vectors V1 and V2 . In this project, PCA is used to reduce the dimensionality of the dataset and identify the most important features that explain the variance in the data. There are several columns in the dataset that are highly correlated, and PCA can help identify these relationships and reduce the dimensionality of the dataset. By reducing the number of features, the model will be less prone to overfitting and will be more interpretable. For example, the Materials and Resins columns are highly correlated. Similarly, the Resin Type and Resin Name columns are highly correlated. PCA will help identify these relationships and reduce the dimensionality of the dataset. References: [1]: https://statisticsbyjim.com/basics/principal-component-analysis/ [2]: https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html . ",
    "url": "/docs/CSCI%205622%20-%20ML/models/pca/#explain-pca-and-distance-metrics",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/pca/#explain-pca-and-distance-metrics"
  },"42": {
    "doc": "Principal Component Analysis <b>(PCA)</b>",
    "title": "Data",
    "content": "The data has already been preprocessed and cleaned using the processes shown in the EDA tab. This dataset contains 180 rows and 8 columns. Here is the first 10 rows of the dataset before PCA: . Vf, % Max. Stress, MPa Freq., Hz E, GPa Max. % Strain Cycles Resin Type Encoded Resin Type 7 50.000 943.000 0.025 126.000 0.720 1.000 0 Epoxy 8 49.000 910.000 0.025 125.000 0.710 1.000 0 Epoxy 9 51.000 966.000 0.025 128.000 0.730 1.000 0 Epoxy 10 52.000 967.000 0.025 131.000 0.710 1.000 0 Epoxy 11 50.000 1066.000 0.025 125.000 0.810 1.000 0 Epoxy 12 51.000 972.000 0.025 130.000 0.690 1.000 0 Epoxy 13 52.000 1067.000 0.025 131.000 0.780 1.000 0 Epoxy 14 51.000 939.000 0.025 134.000 0.710 1.000 0 Epoxy 15 49.000 926.000 0.025 124.000 0.720 1.000 0 Epoxy 16 47.000 1035.000 0.025 119.000 0.830 1.000 0 Epoxy &lt;class 'pandas.core.frame.DataFrame'&gt; Index: 180 entries, 7 to 1059 Data columns (total 8 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Vf, % 180 non-null float64 1 Max. Stress, MPa 180 non-null float64 2 Freq., Hz 180 non-null float64 3 E, GPa 180 non-null float64 4 Max. % Strain 180 non-null float64 5 Cycles 180 non-null float64 6 Resin Type Encoded 180 non-null int64 7 Resin Type 180 non-null object dtypes: float64(6), int64(1), object(1) memory usage: 12.7+ KB . First the data was standardized using StandardScaler: . Table showing the first 10 rows of the dataset after standardization . Then, PCA was applied to the dataset. For n_components=2, the dataset was transformed into 2 principal components. Here is the first 10 rows of the dataset after PCA: . Resin Type is the target variable here. pc1 pc2 Resin Type 0 -2.189 -0.355 Epoxy 1 -2.254 -0.525 Epoxy 2 -2.139 -0.201 Epoxy 3 -2.123 -0.092 Epoxy 4 -2.119 -0.072 Epoxy 5 -2.194 -0.174 Epoxy 6 -2.079 0.144 Epoxy 7 -2.238 -0.220 Epoxy 8 -2.233 -0.495 Epoxy 9 -2.238 -0.443 Epoxy . ",
    "url": "/docs/CSCI%205622%20-%20ML/models/pca/#data",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/pca/#data"
  },"43": {
    "doc": "Principal Component Analysis <b>(PCA)</b>",
    "title": "Code",
    "content": "The code can be accesses on the GitHub repository using the link below: . PCA Code . ",
    "url": "/docs/CSCI%205622%20-%20ML/models/pca/#code",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/pca/#code"
  },"44": {
    "doc": "Principal Component Analysis <b>(PCA)</b>",
    "title": "Results",
    "content": "n_components = 2 . Here is the plot when PCA is applied to the dataset with n_components=2: . The explained variance ratio for the first two principal components is: . Explained Variance Ratio: [0.42561841 0.23724278] . That means, in the 2D dataset, the cumulative explained variance ratio is 0.663. This means that the first two principal components explain 66.3% of the variance in the data. Cumulative Explained Variance Ratio: [0.42561841 0.66286118] . n_components = 3 . Here is the plot when PCA is applied to the dataset with n_components=3: . The explained variance ratio for the first two principal components is: . Explained Variance Ratio: [0.42561841 0.23724278 0.16677749] . That means, in the 3D dataset, the cumulative explained variance ratio is 0.829. This means that the first three principal components explain 82.9% of the variance in the data. Cumulative Explained Variance Ratio: [0.42561841 0.66286118 0.82963867] . To get to 95% explained variance, more calculations were done. n_components = 4 . Explained Variance Ratio: [0.42561841 0.23724278 0.16677749 0.11197639] Cumulative Explained Variance Ratio: [0.42561841 0.66286118 0.82963867 0.94161506] . n_components = 5 . Explained Variance Ratio: [0.42561841 0.23724278 0.16677749 0.11197639 0.0528304 ] Cumulative Explained Variance Ratio: [0.42561841 0.66286118 0.82963867 0.94161506 0.99444546] . So while n_components=4 comes close to 95% explained variance, n_components=5 is first to cross the 95% threshold. The top three eigenvalues are: . Top 3 eigenvalues for n_components = 5: [2.56797697 1.43140894 1.00625523] . The plot showing the explained variance ratio for the first 5 principal components is also shown below: . ",
    "url": "/docs/CSCI%205622%20-%20ML/models/pca/#results",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/pca/#results"
  },"45": {
    "doc": "Principal Component Analysis <b>(PCA)</b>",
    "title": "Principal Component Analysis <b>(PCA)</b>",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/pca/",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/pca/"
  },"46": {
    "doc": "Regression",
    "title": "Regression",
    "content": ". | Overview | Data | Code | Results | . ",
    "url": "/docs/CSCI%205622%20-%20ML/models/regression/",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/regression/"
  },"47": {
    "doc": "Regression",
    "title": "Overview",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/regression/#overview",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/regression/#overview"
  },"48": {
    "doc": "Regression",
    "title": "Data",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/regression/#data",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/regression/#data"
  },"49": {
    "doc": "Regression",
    "title": "Code",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/regression/#code",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/regression/#code"
  },"50": {
    "doc": "Regression",
    "title": "Results",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/regression/#results",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/regression/#results"
  },"51": {
    "doc": "Support Vector Machines <b>(SVM)</b>",
    "title": "Support Vector Machines (SVM)",
    "content": ". | Overview | Data | Code | Results | . ",
    "url": "/docs/CSCI%205622%20-%20ML/models/svm/#support-vector-machines-svm",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/svm/#support-vector-machines-svm"
  },"52": {
    "doc": "Support Vector Machines <b>(SVM)</b>",
    "title": "Overview",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/svm/#overview",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/svm/#overview"
  },"53": {
    "doc": "Support Vector Machines <b>(SVM)</b>",
    "title": "Data",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/svm/#data",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/svm/#data"
  },"54": {
    "doc": "Support Vector Machines <b>(SVM)</b>",
    "title": "Code",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/svm/#code",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/svm/#code"
  },"55": {
    "doc": "Support Vector Machines <b>(SVM)</b>",
    "title": "Results",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/svm/#results",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/svm/#results"
  },"56": {
    "doc": "Support Vector Machines <b>(SVM)</b>",
    "title": "Support Vector Machines <b>(SVM)</b>",
    "content": " ",
    "url": "/docs/CSCI%205622%20-%20ML/models/svm/",
    
    "relUrl": "/docs/CSCI%205622%20-%20ML/models/svm/"
  }
}
